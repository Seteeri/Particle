Traditionally computers have worked by having different applications and
UIs working on both different and similar data with overlapping
operations and functionality. For example, a word processor might include
some image editing operations, and an image editor might provide the
ability to rasterize text.

The idea of Protoform is to invert that model so users have a consistent
interface with less redundancy which is faster to learn. Data today is
not simply numbers and text but much more, and users need an interface
that can cut across all domains.

################################################################################

Generate glyphs for builtin symbols?
- However, if any changes made to glyphs, it would have to be generated and tex
uploaded while running

Could do the same for prose - common english words
https://en.wikipedia.org/wiki/Most_common_words_in_English
- Or analyze this dynamically as user types

--------

For images, load entire image as a texture and then break into tiles
as user works on it, 2,4,8,16,32,64 etc

Make smart process - release tile if user moves to another tile
after a certain amount of time or learn users behavior to calculate the
threshold

################################################################################

text   -> raster (texture)
raster -> text (OCR)

text     -> waveform (text-to-speech)
waveform -> text (voice recognition)

raster   -> waveform (texture)
waveform -> raster (texture)

waveform/spectrogram/bitmap

UNREAL BLUEPRINTS
- Cannot create new nodes with nodes - but we can!

https://news.ycombinator.com/item?id=14482988

################################################################################

Forward... -> Tiled -> Clustered -> Volumetric

See Volume Tiled Forward Shading is based on Tiled and Clustered Forward Shading described by Ola Olsson et. al

################################################################################

https://stackoverflow.com/questions/24839857/wireframe-shader-issue-with-barycentric-coordinates-when-using-shared-vertices
https://catlikecoding.com/unity/tutorials/advanced-rendering/flat-and-wireframe-shading
https://www.reddit.com/r/opengl/comments/3yrv79/simplistic_texture_less_tronlike_3d_with_opengl/

################################################################################

First engine is called Maximum Instancing Engine -> "Maxstance Engine"
- Maximum instances
- Per-instance color/alpha/texture/UVs
  - Make these command line options
- No lighting?
- Post processing effects: motion blur
  - Focused on creating aesthetic appearance over technically correct
...

Use EVVEE for more lighting-based renderings...

################################################################################

How to Design Help System

Use lisp describe method as starting point

User sees an object and wants to do something with it but does not know

Object -> What Function? -> Desired Output

Type help after it and it will print info for that node

- Use a game to teach commands? Like ad destroyer...user has to destroy
nodes as they popup...use augmented reality like pokemon
 - start with basic commands like add/delete nodes, linking nodes, eval,
 swapping nodes, toggling ids

 - Use it to introduce concepts for widgets/GUI so users understand
 how they work - or should animations

 What are the most addictive games?
 Fruit ninja, gem stone game, pokemon go?

 - People like mice because it has two buttons - easy to use and understand
   - They move/hover/point at an object -> press a button -> something happens
 - Mouse's buttons (functions) act on widgets which are generally
 standardized across programs
 - Essentially we want something like that but more powerful
 - Instead of the mouse, we use basic keyboard functions...

 How to combine mouse on one hand, and keyboard on the other?
 Mouse is continous whereas keyboard is discrete
 Keyboard can move 1 step whereas mouse can use physical properties
 like velocity/acceleration

 22. Should pointer target be predecessor or successor?
    - Successor makes sense since pointer is like an arrow

################################################################################

Implement NOTEPAD:

Edit:
* Undo/Redo - TODO
* Cut/Copy/Paste/Delete - Del:YES
* Select All - TODO

Search: TODO
* Find
* Find Next/Prev
* Replace
* Jump To

Options: TODO
* Font - not really feasible right now other than fixed-width/monospace
* Word Wrap - yes
* Line Numbers - yes
* Auto Indent - yes

################################################################################

Never liked the idea of the cursor being inbetween chars...
WHAT IS IT SELECTING???

CLI: Type command -> Enter (no shortcuts)
Emacs: Key chords/bindings + modes [MAX KEYBINDINGS/MODESish]
Archy: Quasimode + type command [MIN KEYBINDINGS/MIN MODES]

US: Type command -> Eval

https://academia.stackexchange.com/questions/109/is-there-any-efficient-non-linear-note-taking-software

MOVEMENT:
1. Link move (semantic move)
2. Euclidean move (discrete steps)
3. Jump move (jump to node)
  -> Requires IDs

   * Diagram:

      h-e-l-l-o---w-o-r-l-d-*

      h-e-l-l-o---w-o-r-l-d
                  *

      Case A
                  w
                  *
      h-e-l-l-o---a-o-r-l-d
                  *

                  w-o-r-l-d
                  * * * * *
      h-e-l-l-o---a-b-c-d-e
                          *

                  w-o-r-l-d
                  * * * * *
                  | | c | |  (c still linked to b and d) !-> b has 3 outputs now...
                  | | * | |
      h-e-l-l-o---a-b-z-d-e
                      *

      Case B - this would need to be explicitly done
               use command like "branch" to move "world"
               to a list

                  *-w-o-r-l-d
                  *
      h-e-l-l-o---a-b-c-d-e
                          *

                  *-w-o-r-l-d
                  *
                  |   *-c-d-e
                  |   *
      h-e-l-l-o---a-b-z
                      *

                  *-w-o-r-l-d
                  *-a
                  *
                  |   *-c-d-e
                  |   *
      h-e-l-l-o---x-b-z
                  *

      If x is deleted, then entire branch moves to undo graph

      Case C - user modifies history; undo will still track changes
               below achieves users intentions

                  *-w-o-r-l-d
                  *
      h-e-l-l-o---a
                  *

                  *-w
                  *
      h-e-l-l-o---a-o-r-l-d
                          *

      alternatives - user retypes it; user copies it

      Case D - this is implicit - "insert-node"
               this actually behaves like overwrite (insert mode) - usually used to maintain spacing
               use alt+<char> for this?

                  w o r l d
                  * * * * *
      h-e-l-l-o---a-b-c-d-e
                          *

      ________________________

                 (* w-o-r-l-d)
                  |
                  *
      h-e-l-l-o---u
                  *

      undo*----->(* w-o-r-l-d)
                 /|\
                  |
                  *
                 /|\
                  ------------------|
      h -> e -> l -> l -> o -> - -> u -> s -> e -> r
                                   /|\
                                    |
                                    *

      * If user wanted to simulate insert, multiple ways:
        - Branch text, link text (del ptr)
        - Cut text/line, insert text, paste text
        - verbose: manually unlink/move everything

                  * w-o-r-l-d
                  *
      h-e-l-l-o---u-s-e-r
                        *

      Relink "world" by typing command:

                  * w-o-r-l-d
                  *
      h-e-l-l-o---u-s-e-r-\n-l-i-n-k---i-d
                          {               }* -> eval

      Eval will unlink newline (input) and link output

      h-e-l-l-o---u-s-e-r-w-o-r-l-d
                                  *
https://softwareengineering.stackexchange.com/questions/224146/how-has-an-increase-in-the-complexity-of-systems-affected-successive-generations/224152

Moreover, let's not forget what we're actually trying to optimize for, which is 
value produced for a given cost. Programmers are way more expensive than 
machines. Anything we do that makes programmers produce working, correct, 
robust, fully-featured programs faster and cheaper leads to the creation of 
more value in the world.

################################################################################

https://news.ycombinator.com/item?id=15466124

No.

I expect this will be a fairly controversial comment, so I want to preface this by saying that I'm a big Lisp fan (just look at my handle). Lisp is my favorite programming language. I've been using it for nearly forty years. My first Lisp was P-Lisp on an Apple II in 1980. And I worked on Symbolics Lisp machines in the 1990s. They were very cool, but there's a reason they failed: general-purpose computing is infrastructure, and the economics of infrastructure are such that having a single standard is the most economical solution, even if that standard is sub-optimal. For better or worse, the standard for general-purpose computing is the C machine.

Because it's general-purpose you certainly can run Lisp on a C machine (just as you could run C on a Lisp machine). You can even do this at the system level. But Lisp will always be at a disadvantage because the hardware is optimized for C. Because of this, C will always win at the system level because at that level performance matters.

But that in and of itself is not the determining factor. The determining factor is the infrastructure that has grown up around the C machine in the last few decades. There is an enormous amount of work that has gone into building compilers, network stacks, data interchange formats, libraries, etc. etc. and they are all optimized for C. For Lisp to be competitive at the system level, nearly all of this infrastructure would have to be re-created, and that is not going to happen. Even with the enormous productivity advantages that Lisp has over C (and they really are enormous) this is not enough to overcome the economic advantages that C has by virtue of being the entrenched standard.

The way Lisp can still win in today's world is not by trying to replace C on the system level, but by "embracing and extending" C at the application level. I use Clozure Common Lisp. It has an Objective-C bridge, so I can call ObjC functions as if they were Lisp functions. There is no reason for me to know or care that these functions are actually written in C (except insofar as I have to be a little bit careful about memory management when I call C functions from Lisp) and so using Lisp in this way still gives me a huge lever that is economically viable even in today's world. I have web servers in production running in CCL on Linux, and it's a huge win. I can spin up a new web app on AWS in just a few minutes from a standing start. It's a Lisp machine, but at the application level, not the system level. My kernel (Linux) and web front end (nginx) are written in C, but that doesn't impact me at all because they are written by someone else. I just treat them as black boxes.

I don't want to denigrate ChrysaLisp in any way. It's tremendously cool. But cool is not enough to win in the real world.

[UPDATE] ChrysaLisp is actually doing the Right Thing with respect to its GUI by
 using a C-library (SDL). But it's trying to re-invent the compiler wheel (and 
 the language design wheel) so that it can run on bare metal and "grow up to be 
 a real Lisp machine" some day, and I think that aspect of the project is a 
 fool's errand. There are already many Lisps that can run on bare metal (ECL was
  specifically designed for that). None of them have succeeding in displacing C, 
  and I believe none ever will because the economic hurdles are insurmountable.

################################################################################

(defun read-64 (buf)
  (let ((u 0))
    (setf (ldb (byte 8 56) u) (aref buf 7))
    (setf (ldb (byte 8 48) u) (aref buf 6))
    (setf (ldb (byte 8 40) u) (aref buf 5))
    (setf (ldb (byte 8 32) u) (aref buf 4))
    (setf (ldb (byte 8 24) u) (aref buf 3))
    (setf (ldb (byte 8 16) u) (aref buf 2))
    (setf (ldb (byte 8 8) u) (aref buf 1))
    (setf (ldb (byte 8 0) u) (aref buf 0))
    u))

(defun read-32 (buf)
   (let ((u 0))
    (setf (ldb (byte 8 24) u) (aref buf 3))
    (setf (ldb (byte 8 16) u) (aref buf 2))
    (setf (ldb (byte 8 8) u) (aref buf 1))
    (setf (ldb (byte 8 0) u) (aref buf 0))
    u))

(defun read-16 (buf)
  (let ((u 0))
    (setf (ldb (byte 8 8) u) (aref buf 1))
    (setf (ldb (byte 8 0) u) (aref buf 0))
    u))

################################################################################

http://www.cs.cmu.edu/afs/cs/academic/class/15210-f15/www/tapp.html#ch:multithreading

Keys to Achieving Parallelism

1. DAG
2. Atomics - minimize usage of locks

Should a task that readds itself create a new task instance or reuse
existing?

https://en.wikipedia.org/wiki/List_of_GNU_Core_Utilities_commands

From
a consumer perspective, the lower abstractions are becoming virtualized
at the convenience of the user, i.e. web browser becoming a platform.
However, the increase in data breaches and security issues have not
completely eroded the OS a key factor.

https://www.reddit.com/r/lisp/comments/9q68y8/has_the_gnu_coreutils_ever_been_implemented_in/

https://www.reddit.com/r/lisp/comments/1fj0qf/lisp_vs_haskell/

https://github.com/BusFactor1Inc/sxc

https://news.ycombinator.com/item?id=13199610

https://www.cliki.net/cl-emacs

Difference between having source code and then eval'ing it like in EMACS
versus having those live data objects accessible in the program like in
Protoform.

https://nullprogram.com/blog/2014/06/29/

Organize functions based on I/O
node -> node
text -> node

https://mattdesl.svbtle.com/drawing-lines-is-hard

http://www.osenkov.com/diplom/contents/1/4/

https://github.com/sile/taomp-sbcl

http://etodd.io/2016/01/12/poor-mans-threading-architecture/

if our engine is a particle system, how to do lighting?

must move as much code to the compute shader as possible

https://www.pvk.ca/Blog/2013/04/13/starting-to-hack-on-sbcl/

http://web.media.mit.edu/~lieber/Lieberary/GC/Realtime/Realtime.html

epoch
bbinit

shell = scripting language for C (processes)

########################################################################

Separate parts become a whole when it is more efficient to do so or
when the interaction of those separate parts becomes more complex than
those parts as a whole.

########################################################################

SASOS

in single address space, the language is unified and thus passing data
is unified. instead of different processes with different formats,
functions pass objects around (ostensibly passing symbols).

unix:
shell process executes C process(es) which communicate through pipes which
are implemented like a file underneath

lisp:
lisp image contains objects which communicate through functions

What we want to do is take that pipeline message passing model and build
on it by using a general purpose language to facilitate it (or build DSL)

We can go further and use DAGs to describe a pipeline that can allow
for parallelization. in this case, we hook objects including fn's
(since fn's are 1st class objects in lisp)

Imagine each C process managing memory along with interpreted languages
with their own GCs versus a single GC managing all the memory and objects.

That includes multiple levels of redundancy increasing complexity and
chances of bugs.

Does this outweigh the disadvantage of a single point of failure?

It is easier to make a single system more stable than multiple systems

However, this single system must be simpler thus easier to understand
and modify than multiple systems, which in the case of lisp vs c, yes

http://wiki.c2.com/?SingleAddressSpaceOperatingSystem

https://www.usenix.org/legacy/event/usenix99/full_papers/deller/deller.pdf

https://news.ycombinator.com/item?id=11187072
http://www.lighterra.com/papers/modernmicroprocessors/

systemd issue is cohesion and coupling - high cohesion and high coupling
high coupling being bad since indicative of high complexitty

Linux kernel-userspace: low cohesion, low coupling

area = group of nodes or leaf?
coupling is like number of edges between two given areas
cohesion is like number of nodes in a given area

the other issue is systemd is more dynamic - able to respond to events
better like hot-plugging...same is becoming more desirable in code...lisp?

GC STILL AN ISSUE...moves nodes to different memory for static objects..weak refs?

ZERO FRAGMENTATION HEAP -> LINEAR HEAP
https://library.softwareverify.com/memory-fragmentation-your-worst-nightmare/

################################################################################

Expand heap = (gc N>0)
Shrink heap = (gc 0)
Heap total size = (heap)
Heap used size = (heap T)
Avail ptr = Cons + adr

Heap address (start) = ?
Avail = ?

Start of frame, cons, store address
- To find avail, could search memory for pointer
End of frame, modify avail to original address

################################################################################

https://emersion.fr/blog/2018/wayland-rendering-loop/

convetionnal: hetero ui - hetero data (overlap)
flip:         homo   ui - hetero data (data/file types)

must provide way to work with all data - open source to start with like OpenDocument

In LISP all data is made of cons cells
In Protoform all trees(UI) are made of nodes
Nodes = cons cells

Buttons are a single node
"Clicking" (state change) is triggered by link/eval (like moving and clicking)
  or poss: pre-link+eval = read->eval, post-link+eval = quote->eval
  this would be like moving over "presses" button

  [code = str; all just bytes->bits->binary in the end ;)]

  general tree -> binary tree (diff data encodings, gen tree lowered to cons/binary tree)
  code         -> data        (diff data encodings, str is lowered to cons/bytes/bits/binary)

  read  = code -> data
  print = data -> code

  quote = data -> code (lets data be code)
  eval  = code -> data (code is what we understand; data is what computer understands)

idea is ops performed on gen tree can afterwards still convert back to cons
str is understood by humans and cons/bytes/bits/binary is understood by machines
- translate both ways is the key to understanding and efficiency/productivity
gen tree is understood by humans and binary tree is understood by lisp interpreter/eval
ui/nodes are code and data
nodes can be transformed to data so that node represents its value (unless quoted)

(setq node 0)

(when <node/data> (exit)) ; triangles = data
-> 0 (numeral)

vs

(when (node/code) (exit)) ; squares = code
-> node (symbol/string)

print = tri -> sq
read = sq -> tri

so node can represent both code and data and user can apply transformers

Link multiple times which cannot be done in binary tree
node=T
-> when
-> if
-> ...

In lisp code, this would just be a symbol refernced by all

: (scl 4)
-> 4

: (setq P (native "@" "malloc" 'N 16))
-> 10851136

: P
-> 10851136

: (struct P 'N (1.0 0.11 0.22))
-> 10851136

: (struct P (1.0 . 2))
-> (1100 2200)


https://stackoverflow.com/questions/17717600/confusion-between-c-and-opengl-matrix-order-row-major-vs-column-major

9.005 Are OpenGL matrices column-major or row-major?

For programming purposes, OpenGL matrices are 16-value arrays with base vectors laid out contiguously in memory. The translation components occupy the 13th, 14th, and 15th elements of the 16-element matrix, where indices are numbered from 1 to 16 as described in section 2.11.2 of the OpenGL 2.1 Specification.

Column-major versus row-major is purely a notational convention. Note that post-multiplying with column-major matrices produces the same result as pre-multiplying with row-major matrices. The OpenGL Specification and the OpenGL Reference Manual both use column-major notation. You can use any notation, as long as it's clearly stated.

Sadly, the use of column-major format in the spec and blue book has resulted in endless confusion in the OpenGL programming community. Column-major notation suggests that matrices are not laid out in memory as a programmer would expect.

################################################################################

* DAG/nodes IPC
  * Interop/conversion between bytes and cons a poss issue!!
    * Render does no additional processing on nodes, simply does memcpy
    * Memcpy certain amount of bytes per frame
      * Perform benchmark; dependent on computer performance characteristics
    * If render simple and mostly C calls and development static, write render
    in C

  * No shared memory? -> Later, use to increase performance as needed with memfd_create
    * Model processes nodes, serializes, puts into queue of render
    * Use socket buffer as message queue, for now...

  * To do multiple processes working on nodes, split total node capacity across
    processors
    * However, processes will need to synchronize with each other
    * To optimize, localize objects by moving between processes
    * Use PicoLisp pool+memfd_create, in this situation
      * Pool uses GIL on the file, although can lock single symbols

  * or...BUILD A GUI AROUND THE EXISTING DATABASE STRUCTURE?
    * Later, when multiple processes need to work on data and synchronize, use
      PL pool (except use memfd_create instead)
  * On top of nodes, build binary trees - lisp code/data


* Implementation
  * Graph = pool
  * Node  = external symbol
  * Modify Node, Commit Node, Notify Render Of Node
  * Number nodes node-1, node-2, node-n... and allocate among processors
    * Or one database per processor
  * Intermediate file write can be removed if memory used directly
    * Ask on mailing list about this

* If we use PicoLisp infrastructure:

  Model
  |_________________________
  |      |       |    |    |
  View-1 View-2  W-1  w-2  w-3

* If we use symbols/classes, then those would represent the GUI parts?

Latency:

Each process has a copy of objects in its local heap. Think of DB as external
heap from app perspective.

It'll be faster if we back it with memfd

* Model write DB
  *
* Render read DB
  * Update nodes when idle...

https://www.mail-archive.com/picolisp@software-lab.de/msg08887.html

################################################################################

Areas
- Have instances of Heap lists
- When wanting to use another, simply change pointers Heap and Avail
- GC remains the same
- But what if object in Heap A points to object in heap B?
  - Would still work but might be inefficient due to possibly different pages
  -> Need copying/moving/compacting solution

- Can simulate areas by processes but would have to copy objects over when
cross references exist - naturally copying/moving/compacting
  - Proc could broadcast request which any node would satisfy
  - Gives concurrent/parallel GC for free
  - Gives compacting for free also - initially copy cost must be paid once upfront

###############################################################################

Rename Protoform to Particle?

User creates cells and builds up nums, syms, cons', nils'

Everything represented as vertices -> functions are the same on every level

- A vertex represents a cons cell
- A cons cell is made of two pointers, called car & cdr
- These two pointers can point to an atom or to another cons cell
  - Car/Cdr is either pointer or binary
- An atom is an encoded pointer, which is a number or a symbol/string(UTF-8)
- Numbers: shortnum fit in single cell (two pointers), bignum consists of multiple cells
- Symbols: CDR=value, CAR= Nil|Name|PList&Name
  - If only name and no plist, then CAR would point to cons cell containing name
  - If bignum, then contains multiple cons cells (list with encoded numbers)
  - If only plist and no name, then CAR would point to a plist with last cons CDR=Nil
    which is where name would normally be

* Use adr to get encoded pointer - decode to get ptr or for num data
  - shift left: NIL = (adr NIL) = 273780<<4 = 4380480
  - left shift = (>> -4 8786312637524)
  - (cons NIL NIL) = 72 215 66 0 0 0 0 0 | 72 215 66 0 0 0 0 0

* Given (setq A (cons 'B 'C))
* Symbol ptr, points to CDR so (sym 'A) = ptr to CDR = ptr of (cons 'B 'C)
* (adr A) = cons address
* (adr 'A) = ?
* quote def: any doQuote(any x) {return cdr(x);}
* Doing (adr (- adr-sym 8)) = infinite loop

################################################################################
################################################################################

         +-----+-----+
         | CAR | CDR |
         +-----+-----+
         TAIL     HEAD

Typing characters produces transient symbol characters with the CDR being shown
by default (the value).

"h" "e" "l" "l" "o"

      +-----------------+
      |                 |
      V                 |
      +-------+-----+   |
      |  'H'  | PTR |---+
      +---+---+-----+

Links represent pointers
- Each char is individual symbol so not linked
- If chars were in a list, there'd be lines

With chars, can move independently since they are different symbols and thus
different memory blocks. If they are to be moved together, they must be packed
and vice versa for the opposite.

User can choose to see CAR/CDR/CONS:
- Atoms
  - Symbols (CDR:VALUE)
    - Internal: show name
      - CDR: NIL (default)
      - CAR: Name, PL (poss)
    - Transient: show name
      - CDR: Ptr to CAR/Symbol
      - CAR: Name in encoded pointer (> word = cons cells/list)
    - Anon: show ptr
      - CDR: NIL
      - CAR: 0 (number in encoded pointer)
        - (= (cons 0 NIL) (box)) : same content at different addresses
    - External: Internal
    - NIL: just another symbol sort of...show NIL/""/()...only defined once
  - Number: show number cells, shortnum=pointer, large numbers=cons
    - largest short number is 1,152,921,504,606,846,976
    - greater than that will be broken into lists
    - have to use struct to get cells (do later)
    - how to differentiate numbers vs chars?
      - either use strings or colors
      - worry about this later
- Cons
  - Pair: show CAR/CDR...for lists it works also recursively


Abstract:

      (default transient symbol)

      +---------------------+
      |                     |
      V                     |
      +--------+--------+   |
      |  'H'   |   PTR  |---+
      +---+----+--------+

  * Single cons cell = contiguous memory of 16 bytes = 2 words
  * Pointers are separate sections of contiguous memory linked together
  * Poss laterally convert between integer-binary-hex (number formats)
  * Basically, type can be derived graphically/structurally
  * Could put CAR/CDR together and color to differentiate (or other method)

EXAMPLE: (any "(list 0 abc \"def\" (box) NIL)") :

  Quantum View:

  Raw Binary


  Element View:

  12345678  87654321  ->  12345678  87654321  ->  12345678  87654321  ->  12345678  87654321  ->  12345678  87654321  ->  12345678  87654321  ->  NIL
  |                       |                       |                       |                       |                       |
  V                       V                       V                       V                       V                       V
  12345678  87654321      12345678  87654321      12345678  87654321      12345678  87654321      12345678  87654321      NIL
  |         (OPAQUE)      |         |             |         |             |         |             |         |
  V                       V         V             V         V             V---------+             V         V
  list                    0         NIL           abc       NIL           "abc"                   0         NIL

  * Opaque pointers, such as pointers to built-in functions - no descendants
  * Ptrs = no descendant
  * Nums/Syms = descendant

  Modifications:
  * Changing pointer digits will cause descendants to regenerate
  * Changing symbol values or numbers will cause predecessors to regenerate
  
  -> Flip model since we work on symbol names
  -> User chooses option for top: Atoms, CAR/CDR, Pointers, Binary


  Composite View (show CAR):

  12345678  ->  12345678  ->  12345678  ->  12345678  ->  12345678  ->  12345678
  |             |             |             |             |             |
  V             V             V             V             V             V
  list          0             abc           "def"         0             NIL

  * Again, an opaque pointer will not have a descendant
  * Cons will show first part only


  Atomic View:

  list -> 0 -> abc -> "def" -> ${12345678} -> NIL [-> NIL]


Numbers vs Pointers:

  12345678  12345678   (cons cell with two opaque pointers)

  12345678  12345678   (cons cell with two numbers)
  |         |
  V         V
  1         2


QUESTIONS:

  * How to show spaces?

################################################################################

Strings = main interface = symbols, so strings = symbols
Symbols are then composed of cons cells...

Strings -> Cons cells
        -> Numbers (bignum=cons cells)

Note, characters are stored in short num (1-7 bytes) or big num (8 bytes)

Convert numbers by typing name vs pressing digits?
-> Numbers faster when sequential but thats less frequent
  - In that case, easier to write program to produce it
-> Numbers faster when only typing numbers, but when mixed,
   typing name faster since fingers remain near home keys

################################################################################

Each vertex has either a texture or a pixel (no texture)

################################################################################

- Default options (for auto pack/glue):
  - Num: prints bytes? ...(see below)
  - Char: user explicitly packs etc
  - Words: auto pack on enter or space (non-graphical characters)
  - Sentences: auto pack on . + non-graphical character
  - Paragraphs: auto pack on tab

- Eval=<mod>+Enter, Read=<mod>+Enter to convert code to data)

Internally, nodes are added to a timeline (maintains ref to them so no GC)
- Question - should users modify timeline directly?

################################################################################

: "X"
-> "X"
:
: (struct (+ (>> -4 (adr '"X")) 0) '(B . 16))
-> (130 5 0 0 0 0 0 0 40 203 209 14 180 127 0 0)

-So CAR and CDR are two different pointers
 ...which is probably two instances of the name then...

 Trans symbol
 - name stored directly in car/left
 - right changes with value, however, it is not nil
 - initially it is not the same as name
   -> name may be contained in another cell

:
: (struct (- (>> -4 (adr "X")) 0) '(B . 16))
-> (130 5 0 0 0 0 0 0 216 221 163 76 236 127 0 0)
:
: (struct 140652874816984 '(B . 16))
-> (216 221 163 76 236 127 0 0 216 221 163 76 236 127 0 0)
:


: (struct (- (>> -4 (adr (box))) 0) '(B . 16))
-> (2 0 0 0 0 0 0 0 72 215 66 0 0 0 0 0)
: (struct (- (>> -4 (adr (box 1))) 0) '(B . 16))
-> (2 0 0 0 0 0 0 0 18 0 0 0 0 0 0 0)
: (struct (- (>> -4 (adr (box 0))) 0) '(B . 16))
-> (2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0)

Anon Symbol:
CAR:  0
CDR: NIL

Transient Symbol:
CAR: name (if name > 7 characters, then ptr to cons cell)
CDR: pointer to CDR of itself (points to symbol)

So CDR points to a cell which points to itself

[name|ptrVAL] ->

How to represent numbers...


ASCII = Character
Number = Character

Alt+ASCII = Type Num
Alt+Number = Type Num

Shift+Enter/Tab = Read
Ctrl+Enter/Tab = Eval

################################################################################

[user@skynet protoform]$ Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called close-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called close-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called open-restricted
Called close-restricted
Called open-restricted
Called open-restricted
Called close-restricted


################################################################################

Possible Bug! -> More like something I don't understand

input.l:

(class +Input)
(dm T ())
(dm test> () T)


main.l:

(load "input.l")

(def '*input)

(de end-main ()
  (test> *input))

(de main ()
    (push '*Bye '(end-main)))


https://news.ycombinator.com/item?id=9981874

JIT inspired by
- Pixie
- El Compilador
- Stalin
- SELF
- LuaJIT


https://en.wikipedia.org/wiki/Self-organizing_list


Launch:
* libinput
* model
* render

libinput connects to model
model connects to render

* make model the server
* libinput and render connect

libinput is one of many
view is one of many
but only one data

OpenGL::

* Triple/NoSync roughly similar to Double/NoSync

https://www.bfilipek.com/2015/01/persistent-mapped-buffers-benchmark.html

LAYOUT 1
========

I/O       CPU         I/O
Input --> Ctrl 1 <--> Model, Render 
      --> Ctrl 2 <--> Model, Render 
      --> Ctrl 3 <--> Model, Render 
      --> Ctrl N <--> Model, Render

INPUT:  GOOD
CTRL:   ????
MODEL:  ????
RENDER: ????

Memcpy Trail
* Ctrl/Serialize, Ctrl/Socket -> Render/GL

* Input epolls for events and forwards them
  * Input not just physically attached devices/peripherals
  * Poss network I/O

* Ctrl
  * Applies functions to data
  * Pulls from Model/Update cache
  * Cache to reduce necessary copies and decrease latency
  * Pushes to Model & Render
  
* Model epolls for memcpy
  * Recvs data from Ctrl and updates memory
  * Broadcast updates
    -> Poss decentralize?
  * Use Redis or some other mechanism/library
    
* Render epolls for memcpy
  * Pushes frame time to Ctrls -> Triggers anim frame generation
  * After pushing frame, Render can either wait or continue

* Supports at least quad-core...seems like a waste to have input/model/render on
own proc
  * Poss assign to same process at the expense of latency to increase bandwith

LAYOUT 2
========

Input --> Model <--> Render

*DOES NOT SCALE!
*Ties input handling and frame handling together

* Model will respond to events through epoll
  * Input handlers write directly to render socket/queue
    * If buffer becomes full, must block recv events until render catches up
    * Input can initialize anim events
  * Frame handlers write to render socket/queue
    * This triggers animation frames to be produced
    * UDS can be implemented to drop animation frames
    * One socket per animation
      * Grouping
* Render will try recv on TCP and recv UDS
  * UDS - one datagram per socket; must process all sockets
  * TCP - iterate through sockets until empty or time limit
  * Memcpy speed is relatively constant which can be used to approximate the
  number of bytes that can be copied per frame = ~192 mb/16.7ms
    * IPC overhead like syscalls to recv/send will be an issue

##################

Animation Example:
* Ctrl: 
  * Loop #0: Recv event->Handle event->Write Render socket
* Render (pre-draw):
  * Loop #1: Send time->Read sock
  * Loop #2: Send time->Read sock
  * Loop #3: Send time->Read sock
  
"When a new packet is received for a UDP socket that has a full receive buffer, 
the new packet is dropped, even when it would be preferrable to drop the 
older (buffered) packets."

* Assume UDP sized for 1 frame
  * UDP buffer size can be increased to accomodate additional frames to get latest N frames
* If render handler sends TCP then render must recv from socket until last data
* Also render, must pull all events first before executing memcpy
* Between Loop #1/2/3, if render gets frame 1 on frame 3 time, then it will still
get frame 1 time, then possibly frame 4 will be loaded next, resulting in the
middle frames being not drawn (dropped).
* Frames can be delayed or dropped
  * Per task
  * If delayed animation will appear too fast or slow...?
    -> Users don't like delayed typing which is annoying
    -> Why? Disorients users as to whats going on - defies expectations
       -> Least Astonishment Principle
* Render only draws so even if the data is not displayed, it still exists and
can be rendered anytime.

https://stackoverflow.com/questions/2963898/faster-alternative-to-memcpy


################################################################################

#{
https://gavv.github.io/articles/unix-socket-reuse/

A socket file is created by bind(2) call.
If the file already exists, EADDRINUSE is returned.

Unlike Internet sockets (AF_INET), Unix domain sockets (AF_UNIX) doesn’t have
SO_REUSEADDR, at least on Linux and BSD. The only way to reuse a socket file
is to remove it with unlink().

There are two bad approaches to deal with this problem:

#1

We could call unlink() just before bind().

The problem is that if we run two instances of our process, the second one will silently remove socket used by the first one, instead of reporting a failure.

Also, there is a race here since the socket can be created by another process between unlink() and bind().

We could call unlink() when the process exits instead.

The problem is that if our process crashes, unlink() will not be called and we’ll have a dangling socket.

#2

Using a lock file

One option is to use a lock file in addition to the socket file.

We’ll use a separate lock file and never call unlink() on it. When a process is going to bind a socket, it first tries to acquire a lock:

If the lock can’t be acquired, it means that another process is holding the lock now, because kernel guarantees that the lock is released if owner process exits or crashes.

If the lock is successfully acquired, we can safely unlink() the socket, because we’re the only owner and no race may occur.

#3
Using abstract namespace sockets

Another option is to use Linux-specific abstract namespace sockets.

To create an abstract namespace socket, set the first byte in the sun_path field of the sockaddr_un to \0. See unix(7). This socket will not be mapped to the filesystem, so it’s not possible to use filesystem permissions or remove it with unlink().

The advantage is that such a socket is automatically removed when the process exits, so there is no problem with socket reusing.

}#


################################################################################

https://datenwolf.net/bl20110930-0001/

My dream graphics system was completely abstract. Creating a window didn't involve selecting visual formats, framebuffer configurations. It was just "a window". Only when actual content is involved I want to tell the rendering subsystem, which color space I use. Ideally all applications worked in a contact color space (e.g. CIE XYZ or Lab), but sending images in some arbitrary color space, together with color profile information. Fonts/Glyphs would be rendered by some layer close to the hardware, to carefully adjust the rasterizing to the output devices properties. And last but not least the whole system should be distributed. Being able to "push" some window from one machine's display, to another machine's (and this action triggering a process migration) would be pinnacle. Imagine you begin writing an email on your smartphone, but you realize you'd prefer using a "usable" keyboard. Instead of saving a draft, closing the mail editor on the phone, transferring the draft to the PC, opening it, editing it there. Imaging you'd simply hold your smartphone besides your PC's monitor a NFC (near field communication) system in phone and monitor detects the relative position, and flick the email editor over to the PC allowing you to continue your edit there. Now imagine that this happens absolutely transparent to the programs involved, that this is something managed by the operating system.

Wayland/X11 - uses window buffer/handles
Emacs - uses text buffer

Particle - uses cons cell (buffer)

http://blog.rongarret.info/2015/05/why-lisp.html
http://blog.rongarret.info/2006/10/top-ten-geek-business-myths.html

Every atom generated is a vertex and lists are represented by edges.
-> For strings, need to traverse name cell
-> For symbols, need to traverse property list and name
-> For numbers, traverse for bignums

http://xahlee.info/UnixResource_dir/writ/lisp_problems.html

Confusing
Lisp's irregular syntax are practically confusing. For example, the difference between (list 1 2 3), '(1 2 3), (quote (1 2 3)) is a frequently asked question. The use of ` , ,@ are esoteric. If all these semantics use the regular syntactical form (f args), then much confusion will be reduced and people will understand and use these features better. For example:

(a . b) ; bad

(. a b) ; good
'(1 2 3) ; bad

(' 1 2 3) ; good
; or
(list-literal 1 2 3) ; good
(setq myListXY `(,@ myListX ,@ myListY)) ; bad

(setq myListXY (` (,@ myListX) (,@ myListY))) ; good
; or
(setq myListXY (eval-parts (splice myListX) (splice myListY))) ; good


Deep Nesting is Rare
The lisp's cons, as a underlying primitive that builds lists, even though a bit cumbersome, but works just fine when data structure used is simple. Even today, with all the Perl, Python, PHP, JavaScript etc langs that deal with lists, vast majority of list usage is just simple flat list, sometimes 2 level of nesting (list of list, list of hash, hash of list). 3 levels of nesting is seldom used, unless it is 3D matrices used mostly in computer graphics or linear algebra applications. Greater than 3 level is rarely seen. Systematic manipulation and exploitation of nested list, such as mapping to leafs, to particular level, transposition by permutation on level, or list structure pattern matching in today's functional langs, etc is hardly ever to be seen. (These are common idioms in so-called array languages. For example, APL, MATLAB, Mathematica.)

So, in general, when you just deal with simple lists, the cumbersomeness of using {cons, car, cdr, caardr, …} for list doesn't really surface. Further, the cons is fundamentally rooted in the language. It's not something that can be easily changed except creating a new language. When there is a specific need in a application, there is a haphazard collection of functions that deal with lists at a higher level.

KIND OF LIKE POINTERS, EH?